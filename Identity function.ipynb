{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identity Function approximation with Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, I tried to approximate Identity function, $f(x) = x$, with a neural network with 1 layer in Julia. I used Knet library and autograd module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Knet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x$ is a vector of sample data with size 5 and $w$ is a vector of weigths with size 2. I initialized them randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×1 Array{Float64,2}:\n",
       "  1.7601 \n",
       " -1.13114"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = randn(5)\n",
    "w = randn(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple predict function for forward pass. $w[1]$ is the weight multiplier and $w[2]$ is the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(w,x) = (w[1] .* x) .+ w[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple loss function to calculate the distance of the prediction to true value. Loss is normalized by a power of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(w,x) = (predict(w,x) .- x).^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where magic happens. Knet's autograd automatically calculates the gradient of loss to its first variable, namely $w$. This way, we can access the update values easily during the Stochastic Gradient Descent phase of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::gradfun) (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradloss = grad(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train function is composed of two loops. The outer loop represents the iterations over all data samples and each full iteration over the data is called **epoch**. The inner loop is to iterate over individual samples. In practice, loss is summed over and weights are updated in few samples, called **batches**. However, for our simple purpose, an update in every data point is OK.\n",
    "\n",
    "Here I used the _gradloss_ function to calculate the gradient values corresponding to weight and bias. Then I update the real weight in the opposite direction of gradient multiplied by a **learning rate** of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train!(w,x)\n",
    "    for epoch = 1:100\n",
    "        for sample in x\n",
    "            w[:] = w[:] - gradloss(w,sample)[:] .* 0.01\n",
    "        end\n",
    "        if(epoch % 20 == 0) println(w) end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training, I printed the value of $w$ for every 20 epochs. The last line is the final for $w$. I expected the optimal value to be [1,0] as $f(x) = 1*x+0 = x$ and my results are pretty close for really shallow sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3162; -0.134838]\n",
      "[1.34641; -0.0134722]\n",
      "[1.10106; -0.0413009]\n",
      "[1.04679; -0.0187434]\n",
      "[1.02162; -0.00861958]\n"
     ]
    }
   ],
   "source": [
    "train!(w,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total training error after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010307501187531232"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss(w,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original values and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.882686,0.836563,-0.229076,0.159088,1.07268]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×5 Array{Float64,2}:\n",
       " -0.676439  0.84603  -0.242648  0.153909  1.08725"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(x)\n",
    "predict(w,x)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created test data to see how well my net performs against unseen data. Here are the test data, predictions and the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.30348; -0.296061; -1.24957]\n",
      "[1.32304 -0.311082 -1.2852]\n",
      "0.0030683044463786714\n"
     ]
    }
   ],
   "source": [
    "test_x = randn(3,1)\n",
    "println(test_x)\n",
    "println(predict(w,test_x)')\n",
    "println(sum(loss(w,test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
